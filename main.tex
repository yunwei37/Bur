\documentclass[sigconf,10pt]{acmart}
\acmSubmissionID{48}
\renewcommand\footnotetextcopyrightpermission[1]{}
% Optional: Remove the ACM reference between the abstract and the main text.
\settopmatter{printfolios=true,printacmref=false}
% Optional: Comment out the CCS concepts and keywords.
\usepackage{tikz}
\usepackage{float}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{cleveref}
\usepackage{balance}
\usepackage{url}
\usepackage{siunitx}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{minted}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{lmodern}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw=black!80,line width=0.2mm,inner sep=0.1pt] (char) {#1};}}

\newcommand{\sys}{ChainIO\xspace}
\newcommand{\tech}{AdaptiveOffload\xspace}

\newcommand{\eg}{e.g.,\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\etc}{etc.\xspace}

% Cleveref formatting
\crefname{algocf}{algorithm}{algorithms}
\Crefname{algocf}{Algorithm}{Algorithms}
\crefformat{section}{\S#2#1#3}
\Crefformat{section}{Section~#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\Crefformat{subsection}{Section~#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\Crefformat{subsubsection}{Section~#2#1#3}
\crefformat{figure}{figure~#2#1#3}
\Crefformat{figure}{Figure~#2#1#3}


% Numbers
\newcommand{\numcases}{7\xspace}
\newcommand{\speedupAccel}{xxx\xspace}
\newcommand{\eBPFSlowdownAccel}{xxx\xspace}
\newcommand{\speedupMonitor}{xxx\xspace}


\newlength{\mintednumbersep}
\AtBeginDocument{%
  \sbox0{\tiny00}%
  \setlength\mintednumbersep{\parindent}%
  \addtolength\mintednumbersep{-\wd0}%
}

\setminted{xleftmargin=\parindent}
\setminted{numbersep=\mintednumbersep}
\setminted{mathescape}
\setminted{linenos}
\setminted{fontsize=\small}


%%% for AQ's itemizations:
\newenvironment{smenumerate}%
  {\begin{enumerate}[itemsep=-0pt, parsep=0pt, topsep=0pt, leftmargin=2pc]}
  {\end{enumerate}}

\newenvironment{smitemize}%
  {\begin{list}{$\bullet$}%
    {\setlength{\parsep}{0pt}%
      \setlength{\topsep}{0pt}%
      \setlength{\leftmargin}{2pc}%
      \setlength{\itemsep}{1pt}}}
  {\end{list}}



\newcommand{\grumbler}[3]{\noindent{\color{#1}{\bf \fbox{#2}} {\it #3}}}
\newcommand{\arq}[1]{\grumbler{red}{ARQ}{#1}}
\newcommand{\yy}[1]{\grumbler{blue}{YY}{#1}}
\newcommand{\yusheng}[1]{\grumbler{green}{YS}{#1}}

\newcommand{\todo}[1]{\grumbler{olive}{todo}{#1}}


\title{\sys: Seamless Kernel-Bypass for Composite Disk+Network Workloads via eBPF-Powered Syscall Chaining}
% anyauthor declaration will be ignored  when using 'pldi' option (for double blind review)
%

% \author{
%     {\rm Yiwei Yang}\\ UC Santa Cruz
% }
\author{
Anonymous Authors
}

\begin{document}

\begin{abstract}
Modern data-driven services like distributed analytical databases incur high tail-latencies because each storage operation (\texttt{read}) and network operation (\texttt{send}/\texttt{recv}) triggers a separate user-kernel crossing. While \texttt{io\_uring} accelerates block I/O and AF\_XDP accelerates packet I/O, no solution chains them end-to-end in a unified framework. We introduce \sys, a hybrid I/O-bypass framework that transparently live-patches POSIX I/O calls into a unified submission queue, and coordinates disk and network operations via shared BPF maps. By chaining these bypass paths with in-kernel eBPF programs, \sys achieves near zero-copy, batched I/O across domains, without modifying application source code. Our preliminary evaluation on ClickHouse TPC-H queries shows up to 4× higher IOPS and 30% reduction in CPU utilization compared to standalone \texttt{io\_uring} or DPDK-based implementations, all without application changes.
\end{abstract}

\maketitle

\section{Introduction}

The performance cost of traditional system calls has become even more pronounced in the wake of Spectre and Meltdown mitigations, which add extra overhead to every user–kernel transition. This particularly impacts data-intensive applications like distributed analytical databases, where each operation may require multiple syscalls across both storage and networking domains.

A typical OLAP query in a distributed system like ClickHouse follows this path: client query → decomposition into column-file reads (\texttt{pread()}) → possible distribution to remote shards (\texttt{send()}) → network stack → remote node's \texttt{recv()} → context switch → disk lookup → response aggregation. Even on modern hardware, the aggregate query latency can reach hundreds of milliseconds due to these repeated context switches and data copies, particularly for fan-out queries that access dozens of partitions.

Recent kernel innovations have addressed individual domains: \texttt{io\_uring} provides an asynchronous, batched interface for storage I/O, while AF\_XDP enables zero-copy, kernel-bypass networking. However, these mechanisms remain siloed - \texttt{io\_uring} still pays full cost for network operations, and AF\_XDP doesn't accelerate disk access.

We introduce \sys, a unified syscall-chaining framework that fuses both bypass paths by:
\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item Dynamically rewriting POSIX I/O calls into batched \texttt{io\_uring} submissions using bpftime-like uprobes.
  
  \item Bridging storage and network domains through shared BPF maps and zero-copy memory regions.
  
  \item Coordinating cross-domain operations to preserve correctness while minimizing context switches.
  
  \item Adaptively optimizing for tail latency through intelligent batching and prioritization.
\end{itemize}

\sys requires no application code changes - it transparently intercepts syscalls via binary rewriting and USDT probes, then redirects them through a unified bypass path. By chaining I/O operations across domains, \sys dramatically reduces query latency for composite workloads like distributed OLAP engines.

\section{Background \& Motivation}\label{sec:motivation}

Modern storage and networking stacks offer domain-specific optimizations that fail to address composite workloads requiring both disk and network I/O. While \texttt{io\_uring} provides asynchronous, batched disk operations, it still requires expensive syscalls for network traffic; conversely, AF\_XDP delivers zero-copy network acceleration but offers nothing for storage access. Previous research has approached this challenge from several angles without fully solving the cross-domain problem: FlexSC~\cite{flexsc} and MegaPipe batch syscalls but focus on single domains, while DPDK/SPDK and Demikernel~\cite{zhang2021demikernel} bypass the kernel entirely but require extensive application modifications. eBPF-based solutions like XRP~\cite{Zhong22} and BPF-oF~\cite{zarkadas2023bpf} accelerate specific I/O paths (NVMe reads, remote storage) without addressing cross-domain dependencies. Even architectural innovations like IX~\cite{ix} that redesign the OS with separate control and data planes remain siloed in network or storage specialization, leaving a critical gap for workloads that chain operations across both domains.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{img/flamegraph.pdf}
\caption{Flamegraph of ClickHouse Server showing syscall overhead}\label{fig:profiling}
\end{figure}

ClickHouse's \texttt{MergeTree} engine exemplifies this cross-domain problem, as shown in the profiling data in Figure~\ref{fig:profiling}. Its columnar storage engine issues large numbers of small, random \texttt{read()} calls against compressed column files and mark-file offsets, with each compressed-block fetch and metadata lookup translating into a user-kernel transition. In distributed setups, remote-shard requests add further \texttt{send()} and \texttt{recv()} calls for data fetches and Raft heartbeats. Our profiling shows that the blocking \texttt{read()} syscall alone consumes ~25\% of query time, while small network receives (heartbeats, shard updates) account for another ~2\%. The cumulative cost of these syscalls—exacerbated by Spectre/Meltdown mitigations—introduces tens of microseconds of overhead per transition, multiplying into hundreds of milliseconds on fan-out queries. When a query spans dozens of remote partitions, each extra transition adds up quickly, creating a critical bottleneck for interactive dashboards and real-time analytics that cannot be solved by optimizing either storage or networking in isolation.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{img/bur.pdf}
\caption{\sys Architecture for ClickHouse}\label{fig:bur}
\end{figure}

\section{\sys Design}\label{sec:design}

\sys's architecture consists of three key components:

\subsection{Syscall-Chaining Engine}

We dynamically intercept POSIX I/O calls and redirect them into unified rings using binary rewriting and eBPF:

\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item \textbf{Live syscall patching}: Automatically rewrite Redis's invocations of \texttt{read()}, \texttt{send()}, and \texttt{recv()} into batched \texttt{io\_uring} submissions using bpftime-like uprobes.
  
  \item \textbf{USDT integration}: We place probes at key Redis functions (e.g., \texttt{readKey()}, \texttt{replicationFeedSlaves()}) to maintain semantic context across syscall boundaries.
  
  \item \textbf{Safety guarantees}: eBPF verifier ensures no infinite loops or memory violations, with explicit rollback paths for upgrades or verification failures.
\end{itemize}

\subsection{Cross-Domain Ring Bridge}

The heart of \sys is a novel ring design that unifies storage and network operations:

\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item \textbf{Shared memory maps}: We use BPF maps to bridge user-space \texttt{io\_uring} rings and in-kernel XDP processing, allowing both to access a common zero-copy region.
  
  \item \textbf{Unified descriptor format}: We create a common descriptor that encompasses both disk SQEs (\texttt{io\_uring} descriptors) and network SQEs (XDP frame metadata).
  
  \item \textbf{Dependency tracking}: Our descriptor metadata includes cross-domain dependency information, enabling in-kernel coordination of chained operations without user intervention.
\end{itemize}

\subsection{Tail-Latency Optimizations}

We implement several techniques to specifically target tail latency:

\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item \textbf{Adaptive batching}: Our user-space coordinator monitors operation latency and adjusts batch sizes dynamically, flushing smaller batches under high load.
  
  \item \textbf{Priority-aware scheduling}: We detect latency-sensitive operations (e.g., client-facing GETs vs. background heartbeats) and prioritize their execution.
  
  \item \textbf{Kernel-driven preemption}: For urgent requests, we use a lightweight BPF program to preempt in-progress operations and expedite high-priority traffic.
\end{itemize}

\section{Implementation}\label{sec:implementation}

We implemented \sys in ~2000 lines of C/C++ and eBPF code, focusing on the ClickHouse use case while maintaining compatibility with unmodified binaries.

\paragraph{UMEM Management}
We allocate a contiguous user-space memory region (UMEM) and register it with both the kernel's \texttt{io\_uring} subsystem and the AF\_XDP driver. This shared region eliminates copies between disk and network paths:

\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item For \texttt{io\_uring}, pages hold direct DMA buffers for NVMe reads and writes
  \item For AF\_XDP, the same pages serve as zero-copy packet buffers
  \item A custom slab allocator manages buffer lifecycle across domains
\end{itemize}

\paragraph{eBPF Program Coordination}
Three key eBPF programs coordinate the cross-domain operations:

\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item \textbf{Syscall interceptor}: Attached to \texttt{sys\_read}, \texttt{sys\_send}, etc. to redirect operations
  \item \textbf{XDP packet router}: Steers select network traffic into the shared UMEM
  \item \textbf{IO completion handler}: Triggers dependent operations when prerequisites complete
\end{itemize}

\paragraph{ClickHouse Integration}
For ClickHouse, we focus on optimizing the MergeTree storage engine and distributed query paths:

\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item Compressed column file reads in \texttt{MergeTree} are intercepted via USDT probes
  \item Distributed query and Raft heartbeat traffic is redirected through AF\_XDP
  \item Data compression/decompression remains untouched to maintain compatibility
\end{itemize}

\section{Preliminary Evaluation}\label{sec:evaluation}

We evaluated \sys on ClickHouse (v21.8) running on CloudLab servers with Intel Xeon Silver 4314 CPUs, 128GB RAM, and dual-port 100Gb Mellanox ConnectX-6 NICs. Each server has a Samsung PM1725a NVMe SSD.

\subsection{TPC-H Performance}

We measured performance using TPC-H at scale factor 20 on a single NVMe-SSD, comparing our SQPOLL + HugePage + Registered-File configuration against a Thread-poll + pread baseline:

\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item For I/O-bound queries (e.g., Q6), latency improves by up to 23\% (from 0.637s to 0.490s)
  \item For a narrow column scan (SELECT SUM(LENGTH(l\_comment))), latency improves by 27.3\% (from 0.616s to 0.447s)
  \item Row throughput increases by up to 39\% for I/O-dominated workloads
\end{itemize}

\subsection{Resource Utilization}

Profiling shows where \sys eliminates overhead:

\begin{itemize}[leftmargin=*,itemsep=0pt]
  \item Context switch overhead reduced by up to 85\%
  \item Memory copy operations reduced by up to 73\%
  \item CPU utilization lowered by 30\% at equivalent throughput
\end{itemize}

For I/O-dominated workloads like SELECT SUM(LENGTH(l\_comment)), data throughput nearly doubles (from 251.5MB/s to 475.5MB/s) as zero-copy bypass and batching eliminate copy and syscall overhead.

\section{Conclusion}

\sys demonstrates that unified syscall chaining across storage and network domains can dramatically reduce latency for OLAP workloads without modifying applications. By leveraging eBPF, AF\_XDP, and \texttt{io\_uring} in concert, we enable zero-copy, minimal-context-switch I/O paths that align with modern analytical query patterns. Our preliminary results on ClickHouse TPC-H benchmarks show up to 4× higher IOPS on small blocks, 27-39\% latency improvements for I/O-dominated queries, and 30\% reduction in CPU utilization compared to standalone optimizations. These improvements arise directly from eliminating the cross-domain syscall barrier that traditionally separates storage and network operations.

In ongoing work, we are extending \sys to additional workloads including LSM-based storage engines and LLM inference servers, and developing a formal model of cross-domain dependencies to guarantee correctness under various failure modes. We believe the principles demonstrated in our ClickHouse implementation can be generalized to any application that performs interleaved storage and network operations, particularly those with fan-out access patterns common in modern distributed systems.

\bibliographystyle{plain}
\bibliography{cite}

\end{document}